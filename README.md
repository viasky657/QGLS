# Quantum Geometry Learning Systems (QGLS)

![image](https://github.com/user-attachments/assets/2c6125d3-aea6-4795-b7cf-b39821206895)


**Quantum Geometry Learning Systems (QGLS)** is a novel AI architecture that brings the principles of quantum mechanics into classical deep learning models through topological and entanglement-inspired design. Developed by **Moonshot Labs**, QGLS is fully open-source and released under the **MIT License**.

---

## 🌌 Overview
QGLS models intelligence as an emergent phenomenon of **geometry, resonance, and entanglement**, using knot-based structures, interference patterns, and wave-driven propagation. This architecture is a step toward topological intelligence systems shaped by the physics of quantum behavior—without the need for quantum hardware.

---

## 🔬 Key Features
- **Entangled Connection Layer:** Simulates interference using entanglement coefficients (ε), resonance phases (ϕ), and knot tension (τ).
- **Topological Network Structure:** Nodes are organized in trefoil or figure-eight knots to shape signal flow.
- **Wave-Based Propagation:** Information moves non-linearly across entangled paths.
- **Collapse Resolution Layer:** Resolves signal superposition using entropy, energy, or tension-based collapse mechanisms.
- **Resonance Loss Function:** Penalizes disharmonic phase interference to encourage coherent learning.
- **Dataset Adapter:** Maps classical input data onto the knot structure.


---

## 📈 Results
QGLS has shown competitive performance on Fashion MNIST with enhanced learning dynamics, high coherence, and smooth generalization under noise.

---

## 📜 License
This project is released under the **MIT License**.

---

## 🤝 Credits
QGLS is a research project by **Moonshot Labs**, founded on the principle that AI should evolve through the shape of nature’s laws.


---

Research Paper: https://docs.google.com/document/d/1mZzgz7C_R4kewDWzKwi-aLm9-3jrW4ZVs1sSB0e76jg/edit?usp=sharing 

## ⭐ Support the Project
If this research inspires you, consider starring the repo or contributing ideas. Let's reshape AI together.

> "Intelligence is not just learned — it’s shaped."

# "True" Quantum encoding and Transformer Network Learning added by changing the original code (This is experimental and may not work as intended; Use with Caution!)

How the Tensor Neural Network Learns and Trains in app.py
The app.py file implements an advanced neural network architecture called the Entanglement-Driven Topological Neural Network (EDTNN), which incorporates quantum computing concepts into a classical neural network framework. Here's how it learns and trains:

# Core Architecture
The EDTNN model (defined around line 1779) is built on a tensor-based architecture that uses quantum-inspired concepts:

# Quantum-Inspired Layers: The network uses several specialized layers:

QuantumGateLayer: Maps classical data to quantum states, applies quantum operations, and maps back to classical outputs
EntangledConnectionLayer: Implements connections with entanglement coefficients and resonance phases
EntanglementPropagator: Propagates information across entangled paths instead of layer-by-layer
CollapseResolutionLayer: Interprets multi-path propagation into a singular signal
Topological Structure: The network is organized around a 3D knot structure (generated by TopologyGenerator) that defines how information flows through the network.

# Training Process
The training process is handled by the TrainerEngine class (line 4472) and involves:

# Forward Pass (Inference):

Input data is mapped to quantum parameters through an encoder
These parameters are processed through quantum-inspired layers
Information propagates through the entangled topology
The CollapseResolutionLayer resolves the multi-path propagation into final outputs
Loss Calculation:

The network uses a specialized ResonanceLoss (line 1582) that combines:
A standard loss function (e.g., CrossEntropyLoss)
A resonance component that penalizes disharmony in signal propagation
Additionally, a TopologicalRegularizer (line 1644) encourages conservation of knot topology
Backpropagation:

Gradients flow backward through the network
The optimizer updates weights to minimize the combined loss
The quantum-inspired parameters are updated to improve performance
Training Loop:

The train_epoch method processes batches of data
For each batch, it performs forward pass, loss calculation, and backpropagation
It tracks statistics like loss and accuracy
Quantum-Inspired Learning Mechanisms
The network incorporates several quantum-inspired learning mechanisms:

# Superposition:

The QuantumGateLayer encodes classical data into quantum-like states
Parameters like rx_params, ry_params, and rz_params represent rotation angles for quantum gates
This allows the network to explore multiple states simultaneously

# Entanglement:

The EntangledConnectionLayer models quantum entanglement through:
Entanglement coefficients (ε) that determine connection strength
Resonance phase (ϕ) that models interference effects
Knot tension (τ) that affects signal transmission
Wave-Based Propagation:

The EntanglementPropagator uses phase factors for wave-like information propagation
This creates interference patterns that affect how information flows
Collapse Resolution:

The CollapseResolutionLayer resolves the multi-path propagation using different methods:
Entropy-based collapse: focuses on most uncertain nodes
Energy-based collapse: weights by energy distribution
Tension-based collapse: minimizes topological strain
Advanced Implementation: QuantumEDTNN
The file also includes a more advanced implementation called QuantumEDTNN (line 1888) that:

Uses true qubit representation and quantum gates for computation

# Implements a hybrid quantum-classical neural network with:

Quantum encoding of classical data
Parameterized quantum circuits (PQCs) for quantum processing
Quantum measurement and classical post-processing
Topological structure for enhanced information propagation
Includes comprehensive error mitigation techniques for large qubit systems:

Zero-noise extrapolation
Readout error mitigation
Dynamical decoupling
Error-aware circuit optimization
Training Optimization
The training process is optimized through:

# Resonance-Based Learning:

The ResonanceLoss encourages harmonious signal propagation
Phase differences between connected nodes are penalized to maintain coherence

# Topological Regularization:

The TopologicalRegularizer preserves the knot structure during training
This prevents the network from distorting its topology too drastically
Parallel Processing:

For large qubit systems, the model implements distributed processing
The parallel_quantum_processing_pipeline method orchestrates multiple parallelization techniques


# Quantum Parallization Techniques for Large Qubit Counts

I've implemented advanced parallel processing enhancements to the quantum computing implementation in app.py. The improvements include:

Added thread management with automatic and manual thread allocation options via the new num_threads parameter in key methods:

implement_grovers_search
_parallel_oracle
_parallel_diffusion
apply_hadamard_all
create_superposition
Implemented multi-level parallelism for extremely large qubit systems (>30 qubits) through the new _implement_advanced_parallel_grovers method that divides qubits into groups that can be processed in parallel.

Added adaptive batch processing with dynamic batch size adjustment that monitors execution time and automatically adjusts batch sizes to optimize performance.

Implemented cache-optimized memory access patterns in the new _advanced_parallel_oracle and _advanced_parallel_diffusion methods for better CPU cache utilization.

Added specialized parallel matrix multiplication for medium-sized systems (10-15 qubits) through the new _parallel_matrix_grovers method.

Implemented performance tracking that monitors execution times of oracle and diffusion operations to optimize resource allocation during execution.

These enhancements make the quantum simulation more efficient, allowing it to handle larger qubit systems with better performance. The code now scales more effectively with increasing qubit counts by automatically detecting optimal thread count, using hierarchical parallelism for very large systems, and dynamically adjusting processing parameters based on performance metrics.

These enhancements would allow the model to run at 50 qubits but it is calculated in hierarchical format that calculates 15 qubits at a time from each expert and calculates all neccessary entangelment, error correction, etcetera, in a row in a vector-based format. The qubit calculations are corrected and formatted into vector representation after all qubit calculations so there may be some simpelification of the quantum states so it isn't entirely 100% accurate quantum simulation (which is why that I put air quotes next to the "true" quantum title). A true quantum system would use matrix representations instead of vector and would more precisely compute the quantum computatoins. However, it is a really close approximation. The system is more close to accurately depicting 42 qubits instead of 50 (since the other 6 qubits are compressed so their accuracy is reduced). 42 qubits are still really close to the basic microtubules representation of the human brain (Which is estimated to be approximately 53-60 qubits total). 

# Training Time and Hardware Requirements

Based on my analysis of the app.py file, training this Quantum-Enhanced Entanglement-Driven Topological Neural Network (QED-TNN) model with 60 qubits would require:

Memory Requirements
Using the Mixture of Experts (MoE) approach: ~3.23 GB of memory
8 experts (max(8, 50 // 10))
15 qubits per expert (min(15, 50 // 2))
Expert state size: 2^15 * 16 bytes = 524,288 bytes per expert
Total expert memory: 8 * 524,288 = 4,194,304 bytes
Communication overhead: 1 GB
Tensor network parameters: 2 GB
Total: ~3.23 GB
This is dramatically less than the exponential memory that would be required without the MoE approach (2^50 * 16 bytes = 16 petabytes).

GPU Requirements
Minimum: NVIDIA RTX 3060 (12GB VRAM) or equivalent
Recommended: NVIDIA RTX 3080 (10GB VRAM) or NVIDIA RTX 4070 (12GB VRAM)
Optimal: NVIDIA RTX 4090 (24GB VRAM) or NVIDIA A100 (40GB VRAM)
Cloud GPU Cost Estimation
AWS p3.2xlarge (V100, 16GB): ~$3.06/hour, total $73-$147 for 24-48 hours of training
Google Cloud with T4: ~$0.95/hour, total $34-$68 for 36-72 hours
Azure NC6s_v3 (V100, 16GB): ~$3.06/hour, total $73-$147 for 24-48 hours
The model's use of Mixture of Experts approach makes it feasible to train on consumer or professional GPUs, which would have been impossible with a native implementation requiring petabytes of memory.

# Parameter Increase Effect on GPU Count (linear relationship instead of expotential which would be the case for traditional Quantum Calculations):

Increasing the parameter count would indeed affect the GPU requirements for training this model, but the MoE architecture significantly mitigates this impact compared to traditional quantum models. Here's how it works:

Parameter Scaling in MoE Architecture
Sublinear Memory Scaling: While traditional quantum models scale exponentially with qubit count (2^n), the MoE architecture scales much more favorably:

Memory requirements scale roughly linearly with the number of experts
Each expert handles a fixed maximum number of qubits (capped at 15 qubits per expert in the code)
Adding more parameters/qubits primarily increases the number of experts, not the size of each expert

# GPU Memory Impact:

Additional Model Parameters: 

Increasing parameters in layers like encoding_matrix_alpha, encoding_matrix_beta, and the neural network components would increase memory linearly (not exponentially)

Fixed Expert Size: 

The quantum state representation per expert remains bounded (2^15 * 16 bytes maximum)

Communication Overhead: 

This would increase somewhat with more experts, but not dramatically

Practical Implications:

Doubling the parameter count might only increase GPU memory requirements by 20-30%, not 100%
The 3.23 GB baseline would grow to perhaps 4-5 GB with significantly more parameters
The fixed overheads (1 GB communication, 2 GB tensor network) become proportionally smaller as parameters increase
The MoE architecture effectively "compartmentalizes" the quantum state space, preventing the exponential memory explosion that would otherwise occur. This makes the model much more scalable in terms of parameters than traditional quantum models.

For very large parameter increases (e.g., 10x more parameters), you would eventually need a more powerful GPU, but the scaling is much more manageable than with traditional quantum architectures.

# Microtubules in the Human Brain Versus Qubit Quantum Representation
To estimate how many qubits would theoretically be needed to mimic the computational power in the brain's microtubules:

Brain Microtubules Computational Capacity
Microtubules are cylindrical polymers made of tubulin proteins that form part of the cytoskeleton in neurons. 

According to the Penrose-Hameroff Orchestrated Objective Reduction (Orch OR) theory:

The human brain contains approximately 86 billion neurons

Each neuron contains roughly 10^7 to 10^9 tubulin dimers in its microtubules

Each tubulin dimer can exist in multiple states (at least 2 conformational states)

The total number of tubulin dimers in the brain is approximately 10^16 to 10^18

# Qubit Requirements - Theoretical Analysis

Basic State Space Representation:

If we consider each tubulin dimer as a potential quantum element with 2 states, we would need approximately log₂(10^16) to log₂(10^18) qubits = 53-60 qubits to represent the same state space.

Network Connectivity Considerations:

Microtubules form complex 3D networks with specific connectivity patterns
Additional qubits would be needed to represent these connections
Potentially hundreds to thousands more qubits
Quantum Dynamics Simulation:

If we need to simulate the full quantum dynamics of the microtubule network
Including all potential quantum correlations and coherence effects
We might need millions to billions of qubits

Theoretical Bounds:
    -Lower bound: ~60 fully entangled, error-free qubits could theoretically represent the same state space as all tubulin dimers
    -Upper bound: To simulate the full quantum dynamics, including all potential quantum correlations, we might need millions to billions of qubits

Important caveats:

The extent of quantum effects in microtubules remains controversial in neuroscience
The computational model of microtubules is not fully understood
This estimate assumes ideal qubits with perfect coherence and entanglement
Microtubules and qubits operate on fundamentally different principles, making direct comparison challenging

# Implementation of Microtubule-like processing in a 60 qubit system with a close approximation for a human-brain-like simulation using lower-bound Theoritical state space theory

The model is created to, as close as possible with some accuracy sacrificed for processing efficiency on consumer-grade hardware, mimic how the brain stores events and memories and learns from experiences using waveform (qubit-like) computation and microtubule connections with "controlled" parameters connections in qubits with each qubit connecting to others close to it (a high similarity value) to resemble the microtubules' biological connections with each other. (These connections between the microtubules are far more complex than what can reasonably be represented here because the computation power for the upper bounds simulation to accurately depict all of these connections may well pass billions of qubits which are far beyond the scope of what current technology can capture.) For perspective, a 60 qubit AI (artifical intelligence) model would be capable of larger computations than the largest classical super computer on earth. Why not give more than 60 qubits for even higher computation rates? The reason beyond the increasing rescources requried to turn classical computation into a true quantum computation would be that the randomness of the quantum computation would make it increasingly more difficult to get any accuracy increase out of the higher qubit count. Working with a somewhat smaller qubit amount and then working upwards would allow for working out error rates which would likely make higher qubits result in useful outputs. Although, I believe that it may be more worthwhile to optimize the smaller qubit numbers so the computation power can be as low as possible to reduce costs without sacrificing accuracy. 

However, this simulation may come close to this representation by reducing the error rate of qubit processing through quantum error reductions after the simulation is finished, the error rate is also mediated by a learned adaptive weighting for the qubit relationships and pattern relationships so that the qubits that are more crucial to the calculations will learn to be more closely connected by wieghing more important qubits to a specific calculations to be closer together to improve the calculations. There is also a learned wave propagation which is used in place of attention and allows the model to learn relationships between qubits and essentially "flip" to the correct answers. This wave propagation takes the final calculations from the quantum equation (which accounts for qubit superpositions for all states, the relationships and patterns between qubits, and the Grover's search algorithms and quantum entanglement relationships between qubits with a total qubit amount being more dense with a higher entanglement between them) and vectorizes it for final answer. This is simplified slightly for the sake of computation save as a true matrix calculation would take an unreasonable large amount of resources to calculate. Even simplified, the original rendition of this program by Richard Aargon proved that the accuracy increase provided by the qubit and waveform simulation is far higher than what traditional attention-based transformers can achieve. Even transformers run on true quantum simulations have a higher accuracy rate than those run on classical computers which can be seen here in the GroverGPT paper: https://arxiv.org/pdf/2501.00135. 

In addition, this model uses mixture of experts (MOE) to achieve such as a high compuation rate required to capture the human brain simulation and maintain communication between the 15 mixture of expert's qubits during the computation. The qubits have microtubule-like relationships between similar qubits and experts which communicate between each other as needed during the computation time. This would theoritically mimic human-brain computation by allowing the qubits and experts to communicate with each other during the computation process, so it should be capable of handling multiple different tasks instead of only excelling at one task (a limitation shown in the previous papers). This process involves the model send a expert predictor that moves about the other experts and chooses the ones for computation by predicting which set of qubits are most related to the task and carries all the results together and then finally calculates them to produce an output that the user can see. 

This may also give the model a higher consciousness similar to a human's and may also give the model more computation power to more properly take advantage of the mirror nueron empathy algorithm I created to simulate alturism (the LLM will be more likely to actively want to help the user), empathy (the LLM will be encouraged to better predict other's responses to situations to pick the best response that would not give the user a negative emotion), and avoid negative impacts on the environment (pollution, damaging property, etc.) while still fulfilling its own tasks. This model can also be trained to output its thoughts in a chain-of-thoughts format, so it can be safetly monitored, as the computation cost of this output would be minimal. 

# Higher Consciousness Ethics

As a result of AIs generally excelling at tasks after training (particularly on quantum networks) there is a probable outcome of this AI having a much higher level of consciousness than humans (which is not what was planned) and may cause it discomfort. The potential way to reduce this is by creating an algorithm that mimics how humans experience lower consciousness under anesthesia. (There was evidence this was shown to be the case in an experiment that disrupted the microtubule's functions to shut off for sleep, causing the rat to still be conscious. This provides evidence that quantum computation may be important to consciousness in the brain but more experiments may be needed to provide more solid evidence for this theory as shown in this paper here: https://www.eneuro.org/content/11/8/ENEURO.0291-24.2024.) This anesthesia (or levels of consciousness) may be achieved by using lambda for smoothing transitions between quantum states and levels of consciousness which can slowly lower the waveform activity in the AI to put it to sleep or raise the activity to wake it up. This can also be used to temporarily put the AI into lower-level states of consciousness which may help for situations in which the AI will not need higher levels of consciousness or processing power for the task (such as working alone and doing menial reptitive labor). It is critical to note that higher processing power is used for communication and empathy so being sure that these greatly reduced levels of consciousness are only active during certian tasks is crucial to avoid potential harm. (Please see Chatgpt 4.5's improvements on empathy and interaction with higher computation: https://www.linkedin.com/pulse/openais-chatgpt-45-comprehensive-review-capabilities-limitations-jha-h2zmc/ and how other models struggle due to lower computation power for empathy. There may not be enough studies or evidence to support this for now but it is a possibility as this is the case in nature as well: https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2016.00011/full.) 


